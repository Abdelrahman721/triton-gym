{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e84b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d2f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "def get_autotune_configs():\n",
    "    return [\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        # Good config for fp8 inputs.\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4)\n",
    "    ]\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_configs(),\n",
    "    key= ['M', 'N', 'K']\n",
    ")\n",
    "@triton.jit\n",
    "def mm_kernel(a_ptr, b_ptr, c_ptr,\n",
    "              M, N, K,\n",
    "              am_stride, ak_stride,\n",
    "              bk_stride, bn_stride,\n",
    "              cm_stride, cn_stride,\n",
    "              BLOCK_SIZE_M: tl.constexpr,\n",
    "              BLOCK_SIZE_N: tl.constexpr,\n",
    "              BLOCK_SIZE_K: tl.constexpr,\n",
    "              GROUP_SIZE_M: tl.constexpr,\n",
    "              ):\n",
    "    pid          = tl.program_id(0)\n",
    "    num_programs = tl.num_programs(0)\n",
    "\n",
    "    n_blocks          = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    m_blocks          = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_per_group = GROUP_SIZE_M * n_blocks\n",
    "    grp_idx           = pid // num_pid_per_group\n",
    "    m_group_size      = min(m_blocks - grp_idx * GROUP_SIZE_M, GROUP_SIZE_M)\n",
    "\n",
    "    group_m = grp_idx * GROUP_SIZE_M * BLOCK_SIZE_M\n",
    "    pid_m   = group_m + BLOCK_SIZE_M * (pid % m_group_size)\n",
    "    pid_n   = BLOCK_SIZE_N * ((pid % num_pid_per_group) // m_group_size)\n",
    "\n",
    "    am_offsets = (pid_m + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    bn_offsets = (pid_n + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    k_offsets  = tl.arange(0, BLOCK_SIZE_K)\n",
    "    \n",
    "    a_ptrs = a_ptr + (am_offsets[:, None] * am_stride + k_offsets[None, :] * ak_stride)\n",
    "    b_ptrs = b_ptr + (k_offsets[:, None] * bk_stride + bn_offsets[None, :] * bn_stride)\n",
    "\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in tl.range(0, K, BLOCK_SIZE_K):\n",
    "        a_block = tl.load(a_ptrs, mask=k_offsets[None, :] < (K - k), other=0.0)\n",
    "        b_block = tl.load(b_ptrs, mask=k_offsets[:, None] < (K - k), other=0.0)\n",
    "\n",
    "        accumulator = tl.dot(a_block, b_block, accumulator)\n",
    "\n",
    "        a_ptrs += BLOCK_SIZE_K * ak_stride\n",
    "        b_ptrs += BLOCK_SIZE_K * bk_stride\n",
    "\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # write back the block of the output matrix C with masks\n",
    "    cm_offsets = pid_m + tl.arange(0, BLOCK_SIZE_M)\n",
    "    cn_offsets = pid_n + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + (cm_offsets[:, None] * cm_stride + cn_offsets[None, :] * cn_stride)\n",
    "    c_mask = (cm_offsets[:, None] < M) & (cn_offsets[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc92b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b, activation=\"\"):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    mm_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af250a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output_with_fp16_inputs=tensor([[  0.8882, -25.5312,  12.9375,  ...,  -0.1477,  -8.3750,  -5.4609],\n",
      "        [ 22.3438, -14.1719, -17.5312,  ..., -28.3906, -22.6406,  28.9219],\n",
      "        [-33.6875,   1.8291, -22.8438,  ...,   9.2578, -52.2812,  12.8750],\n",
      "        ...,\n",
      "        [ 14.2031, -54.0312,  10.4609,  ..., -13.9531, -14.6953,  -2.7188],\n",
      "        [-33.2500,  29.1094, -20.9375,  ...,   4.3203, -14.3906, -10.8672],\n",
      "        [-18.5938,  22.7188, -30.7500,  ...,  -9.0312,  -6.8281,  -1.4648]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp16_inputs=tensor([[  0.8882, -25.5312,  12.9375,  ...,  -0.1477,  -8.3750,  -5.4609],\n",
      "        [ 22.3438, -14.1719, -17.5312,  ..., -28.3906, -22.6406,  28.9219],\n",
      "        [-33.6875,   1.8291, -22.8438,  ...,   9.2578, -52.2812,  12.8750],\n",
      "        ...,\n",
      "        [ 14.2031, -54.0312,  10.4609,  ..., -13.9531, -14.6953,  -2.7188],\n",
      "        [-33.2500,  29.1094, -20.9375,  ...,   4.3203, -14.3906, -10.8672],\n",
      "        [-18.5938,  22.7188, -30.7500,  ...,  -9.0312,  -6.8281,  -1.4648]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n",
      "triton_output_with_fp8_inputs=tensor([[ -4.0352,   2.4062,  -7.8008,  -5.6367,   1.3447,   0.2974,  -2.3926,\n",
      "           2.3613,   9.0469,   4.1172,   9.2031,  -0.8486,  -3.3906,  -0.0967,\n",
      "          -2.0430,  -5.4883],\n",
      "        [ -4.5547,   6.4531,  -0.0718,   3.2480,  -2.5508,   1.6963,   1.1523,\n",
      "           5.0586,   2.6484,  -3.2930,  -3.5156,  -2.1328,  -4.4883,   6.2539,\n",
      "           0.4702,  -2.9336],\n",
      "        [ -2.8438,  -2.7070,   0.7861,   0.3298,  -4.8320,  -3.3594,   3.5273,\n",
      "           1.4258,  -3.3066,  -3.1191,   1.2002,  -4.2891,   4.7500,   0.2329,\n",
      "          -5.7109,   3.3008],\n",
      "        [ -0.4004,   1.3174,   2.2012,  -4.8828,   0.2534,   1.9756,  -0.6943,\n",
      "           5.1680,  -1.4150,  -1.6787,   2.2891,  -1.9404,   0.2246,  -0.8701,\n",
      "          -1.4980,  -2.2090],\n",
      "        [  0.2725,  -2.2715,   6.0352,   7.8320,   0.1265,  -1.3154,   0.0225,\n",
      "           0.8535,  -6.4609,   2.3535,  -6.2500,  -0.4233,   0.1487,  -4.1367,\n",
      "           2.6465,  -1.1611],\n",
      "        [ -4.4727,   6.3594,  -9.7969,  -1.3105,  -4.3594,   3.2578,   0.2847,\n",
      "          -0.6191,  10.0078,   2.1562,  11.1406,  -2.0742,  -2.2988,  -0.4458,\n",
      "          -2.9023,  -0.8403],\n",
      "        [  0.0645,  -4.5820,   3.4219,  -1.0488,   2.7188,  -3.9336,   4.3320,\n",
      "          -4.2031, -11.9688,   1.0029,  -3.7832,   0.8145,   3.4883,  -2.3711,\n",
      "          -4.3828,   4.5391],\n",
      "        [ -0.5840,  -1.6543,   2.6289,  -2.6406,  -1.6387,  -5.6289,   1.3193,\n",
      "           5.8125,  -2.8809,  -3.3203,  -6.2852,   4.1484,   2.7266,   2.4141,\n",
      "          -1.8145,   0.4688],\n",
      "        [ -2.7090,   2.8906,  -6.0820,   6.5703,   0.3291,   2.9805,  -1.0039,\n",
      "          -5.3047,   4.3125,   4.3945,   6.0273,  -4.3125,  -4.9961,   3.4727,\n",
      "           4.0273,  -1.0625],\n",
      "        [ -2.8848,   7.1094,   2.4336,   0.8516,  -2.9688,   2.5586,  -3.0547,\n",
      "           0.2910,   3.2520,  -0.0533,   6.5703,  -2.0918,  -4.4922,   0.2051,\n",
      "           1.2568,   0.5859],\n",
      "        [ -2.6523,  -2.1172,  -9.5625,  -4.6328,   0.3413,  -3.4883,   1.5742,\n",
      "          -4.8828,   2.1562,  -1.9492,   0.0400,   0.5410,  -2.3359,   4.1172,\n",
      "           4.3555,   5.6523],\n",
      "        [  1.4248,  -5.0312,   0.7930,   6.5156,   2.5234,  -0.4485,  -2.1309,\n",
      "          -5.7578,   1.1016,   3.9570,   0.2695,   0.2749,   4.0430,  -1.6406,\n",
      "           3.8633,   2.0273],\n",
      "        [ -0.2139,  -4.2422,  -7.1250,  -2.7852,   5.6836,  -2.5215,   2.0879,\n",
      "          -1.7627,  -6.3125,   4.5508,  -2.6816,  -0.2900,  -3.7129,   0.5200,\n",
      "           2.8262,  -1.9570],\n",
      "        [ -2.0801,   6.1406,   0.2188,   9.5625,  -0.4917,   4.7500,  -2.8262,\n",
      "          -4.9297,   9.5625,   4.0000,   0.6592,   2.9922,  -3.8594,  -3.9102,\n",
      "           5.2148,   0.9121],\n",
      "        [ -3.0684,   4.8008,  -3.4453,  -8.3906,  -0.2676,  -1.9043,   0.4219,\n",
      "          -2.9766,  -1.4492,   0.4707,   5.2891,   1.9121,  -3.3008,  -0.7715,\n",
      "          -3.7695,   4.2852],\n",
      "        [ -2.7051,  10.4297,  -4.4609,   4.9883,  -8.9141,   7.7852,   0.9443,\n",
      "           8.8750,   5.5430,  -3.7539,   2.0527, -10.3047, -11.9609,   3.2988,\n",
      "           4.3750,  -8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp8_inputs=tensor([[ -4.0352,   2.4062,  -7.8008,  -5.6367,   1.3447,   0.2974,  -2.3926,\n",
      "           2.3613,   9.0469,   4.1172,   9.2031,  -0.8486,  -3.3906,  -0.0967,\n",
      "          -2.0430,  -5.4883],\n",
      "        [ -4.5547,   6.4531,  -0.0718,   3.2480,  -2.5508,   1.6963,   1.1523,\n",
      "           5.0586,   2.6484,  -3.2930,  -3.5156,  -2.1328,  -4.4883,   6.2539,\n",
      "           0.4702,  -2.9336],\n",
      "        [ -2.8438,  -2.7070,   0.7861,   0.3298,  -4.8320,  -3.3594,   3.5273,\n",
      "           1.4258,  -3.3066,  -3.1191,   1.2002,  -4.2891,   4.7500,   0.2329,\n",
      "          -5.7109,   3.3008],\n",
      "        [ -0.4004,   1.3174,   2.2012,  -4.8828,   0.2534,   1.9756,  -0.6943,\n",
      "           5.1680,  -1.4150,  -1.6787,   2.2891,  -1.9404,   0.2246,  -0.8701,\n",
      "          -1.4980,  -2.2090],\n",
      "        [  0.2725,  -2.2715,   6.0352,   7.8320,   0.1265,  -1.3154,   0.0225,\n",
      "           0.8535,  -6.4609,   2.3535,  -6.2500,  -0.4233,   0.1487,  -4.1367,\n",
      "           2.6465,  -1.1611],\n",
      "        [ -4.4727,   6.3594,  -9.7969,  -1.3105,  -4.3594,   3.2578,   0.2847,\n",
      "          -0.6191,  10.0078,   2.1562,  11.1406,  -2.0742,  -2.2988,  -0.4458,\n",
      "          -2.9023,  -0.8403],\n",
      "        [  0.0645,  -4.5820,   3.4219,  -1.0488,   2.7188,  -3.9336,   4.3320,\n",
      "          -4.2031, -11.9688,   1.0029,  -3.7832,   0.8145,   3.4883,  -2.3711,\n",
      "          -4.3828,   4.5391],\n",
      "        [ -0.5840,  -1.6543,   2.6289,  -2.6406,  -1.6387,  -5.6289,   1.3193,\n",
      "           5.8125,  -2.8809,  -3.3203,  -6.2852,   4.1484,   2.7266,   2.4141,\n",
      "          -1.8145,   0.4688],\n",
      "        [ -2.7090,   2.8906,  -6.0820,   6.5703,   0.3291,   2.9805,  -1.0039,\n",
      "          -5.3047,   4.3125,   4.3945,   6.0273,  -4.3125,  -4.9961,   3.4727,\n",
      "           4.0273,  -1.0625],\n",
      "        [ -2.8848,   7.1094,   2.4336,   0.8516,  -2.9688,   2.5586,  -3.0547,\n",
      "           0.2910,   3.2520,  -0.0533,   6.5703,  -2.0918,  -4.4922,   0.2051,\n",
      "           1.2568,   0.5859],\n",
      "        [ -2.6523,  -2.1172,  -9.5625,  -4.6328,   0.3413,  -3.4883,   1.5742,\n",
      "          -4.8828,   2.1562,  -1.9492,   0.0400,   0.5410,  -2.3359,   4.1172,\n",
      "           4.3555,   5.6523],\n",
      "        [  1.4248,  -5.0312,   0.7930,   6.5156,   2.5234,  -0.4485,  -2.1309,\n",
      "          -5.7578,   1.1016,   3.9570,   0.2695,   0.2749,   4.0430,  -1.6406,\n",
      "           3.8633,   2.0273],\n",
      "        [ -0.2139,  -4.2422,  -7.1250,  -2.7852,   5.6836,  -2.5215,   2.0879,\n",
      "          -1.7627,  -6.3125,   4.5508,  -2.6816,  -0.2900,  -3.7129,   0.5200,\n",
      "           2.8262,  -1.9570],\n",
      "        [ -2.0801,   6.1406,   0.2188,   9.5625,  -0.4917,   4.7500,  -2.8262,\n",
      "          -4.9297,   9.5625,   4.0000,   0.6592,   2.9922,  -3.8594,  -3.9102,\n",
      "           5.2148,   0.9121],\n",
      "        [ -3.0684,   4.8008,  -3.4453,  -8.3906,  -0.2676,  -1.9043,   0.4219,\n",
      "          -2.9766,  -1.4492,   0.4707,   5.2891,   1.9121,  -3.3008,  -0.7715,\n",
      "          -3.7695,   4.2852],\n",
      "        [ -2.7051,  10.4297,  -4.4609,   4.9883,  -8.9141,   7.7852,   0.9443,\n",
      "           8.8750,   5.5430,  -3.7539,   2.0527, -10.3047, -11.9609,   3.2988,\n",
      "           4.3750,  -8.1016]], device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device=DEVICE, dtype=torch.float16)\n",
    "triton_output = matmul(a, b)\n",
    "torch_output = torch.matmul(a, b)\n",
    "# torch.set_printoptions(profile=\"full\")\n",
    "print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
    "print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
    "# torch.set_printoptions(profile=\"default\")\n",
    "if torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0.0):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")\n",
    "\n",
    "TORCH_HAS_FP8 = hasattr(torch, \"float8_e5m2\")\n",
    "if TORCH_HAS_FP8 and is_cuda():\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((16, 16), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((16, 16), device=DEVICE, dtype=torch.float16)\n",
    "    a = a.to(torch.float8_e5m2)\n",
    "    # pre-transpose b for efficiency.\n",
    "    b = b.T\n",
    "    b = b.to(torch.float8_e5m2)\n",
    "    triton_output = matmul(a, b)\n",
    "    torch_output = torch.matmul(a.to(torch.float16), b.to(torch.float16))\n",
    "    print(f\"triton_output_with_fp8_inputs={triton_output}\")\n",
    "    print(f\"torch_output_with_fp8_inputs={torch_output}\")\n",
    "    if torch.allclose(triton_output, torch_output, atol=0.125, rtol=0):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        print(\"❌ Triton and Torch differ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ff1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_lib = 'cuBLAS' if is_cuda() else 'rocBLAS'\n",
    "\n",
    "configs = []\n",
    "for fp8_inputs in [False, True]:\n",
    "    if fp8_inputs and (not TORCH_HAS_FP8 or not is_cuda()):\n",
    "        continue\n",
    "    configs.append(\n",
    "        triton.testing.Benchmark(\n",
    "            x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "            x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`\n",
    "            line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "            # Possible values for `line_arg`\n",
    "            # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "            line_vals=[\"triton\"] if fp8_inputs else [ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "            line_names=[\"Triton\"] if fp8_inputs else [ref_lib, \"Triton\"],  # Line styles\n",
    "            styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "            ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "            plot_name=\"matmul-performance-\" +\n",
    "            (\"fp16\" if not fp8_inputs else \"fp8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "            args={\"fp8_inputs\": fp8_inputs},\n",
    "        ))\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider, fp8_inputs):\n",
    "    a = torch.randn((M, K), device=DEVICE, dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device=DEVICE, dtype=torch.float16)\n",
    "    if TORCH_HAS_FP8 and fp8_inputs:\n",
    "        a = a.to(torch.float8_e5m2)\n",
    "        b = b.T\n",
    "        b = b.to(torch.float8_e5m2)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
