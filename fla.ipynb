{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2124ea5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 361\u001b[0m\n\u001b[1;32m    358\u001b[0m     atol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-2\u001b[39m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(ref_O, tri_out, atol\u001b[38;5;241m=\u001b[39matol, rtol\u001b[38;5;241m=\u001b[39mrtol)\n\u001b[0;32m--> 361\u001b[0m \u001b[43mtest_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_HEADS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHEAD_DIM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 354\u001b[0m, in \u001b[0;36mtest_op\u001b[0;34m(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, causal, dtype)\u001b[0m\n\u001b[1;32m    351\u001b[0m ref_O \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(P, V)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# triton implementation\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m tri_out \u001b[38;5;241m=\u001b[39m \u001b[43mTritonAttention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# compare\u001b[39;00m\n\u001b[1;32m    357\u001b[0m rtol \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[3], line 275\u001b[0m, in \u001b[0;36mTritonAttention.forward\u001b[0;34m(ctx, Q, K, V, causal, softmax_scale)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# M is the logsumexp for the backward pass, one for each query\u001b[39;00m\n\u001b[1;32m    271\u001b[0m M \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[1;32m    272\u001b[0m     (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device\u001b[38;5;241m=\u001b[39mQ\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    273\u001b[0m )\n\u001b[0;32m--> 275\u001b[0m \u001b[43m_attn_fwd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43msoftmax_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoftmax_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mO\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_Q_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_Q_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_Q_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_Q_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_K_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_K_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_K_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_K_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_V_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_V_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_V_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_V_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_O_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_O_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_O_seq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_O_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNUM_HEADS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mHEAD_DIM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEAD_DIM_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSTAGE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(Q, K, V, O, M)\n\u001b[1;32m    306\u001b[0m ctx\u001b[38;5;241m.\u001b[39mgrid \u001b[38;5;241m=\u001b[39m grid\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/runtime/jit.py:347\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/runtime/autotuner.py:192\u001b[0m, in \u001b[0;36mAutotuner.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    191\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 192\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bench(\u001b[38;5;241m*\u001b[39margs, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    193\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/runtime/autotuner.py:192\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    190\u001b[0m pruned_configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune_configs(kwargs)\n\u001b[1;32m    191\u001b[0m bench_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 192\u001b[0m timings \u001b[38;5;241m=\u001b[39m {config: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[1;32m    193\u001b[0m bench_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench_time \u001b[38;5;241m=\u001b[39m bench_end \u001b[38;5;241m-\u001b[39m bench_start\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/runtime/autotuner.py:170\u001b[0m, in \u001b[0;36mAutotuner._bench\u001b[0;34m(self, config, *args, **meta)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_hook(full_nargs, exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OutOfResources, CompileTimeAssertionFailure, PTXASError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/testing.py:145\u001b[0m, in \u001b[0;36mdo_bench\u001b[0;34m(fn, warmup, rep, grad_to_none, quantiles, return_mode)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m return_mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    143\u001b[0m di \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mget_device_interface()\n\u001b[0;32m--> 145\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m di\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    148\u001b[0m cache \u001b[38;5;241m=\u001b[39m runtime\u001b[38;5;241m.\u001b[39mdriver\u001b[38;5;241m.\u001b[39mactive\u001b[38;5;241m.\u001b[39mget_empty_cache_for_benchmark()\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/runtime/autotuner.py:156\u001b[0m, in \u001b[0;36mAutotuner._bench.<locals>.kernel_call\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_hook(full_nargs)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/runtime/jit.py:569\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    568\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constexprs, attrs)\n\u001b[0;32m--> 569\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    570\u001b[0m kernel_cache[key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_hook(key, signature, device, constexprs, options, [attrs], warmup, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/compiler/compiler.py:284\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    282\u001b[0m use_ir_loc \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE_IR_LOC\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ext, compile_ir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stages\u001b[38;5;241m.\u001b[39mitems())[first_stage:]:\n\u001b[0;32m--> 284\u001b[0m     next_module \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     ir_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (fn_override_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (full_name \u001b[38;5;241m:=\u001b[39m fn_override_manager\u001b[38;5;241m.\u001b[39mget_file(ir_filename)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/backends/nvidia/compiler.py:450\u001b[0m, in \u001b[0;36mCUDABackend.add_stages.<locals>.<lambda>\u001b[0;34m(src, metadata)\u001b[0m\n\u001b[1;32m    448\u001b[0m stages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mttir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_ttir(src, metadata, options)\n\u001b[1;32m    449\u001b[0m stages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mttgir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_ttgir(src, metadata, options, capability)\n\u001b[0;32m--> 450\u001b[0m stages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_llir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapability\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m stages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_ptx(src, metadata, options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39march)\n\u001b[1;32m    452\u001b[0m stages[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcubin\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_cubin(src, metadata, options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39march)\n",
      "File \u001b[0;32m~/triton-gym/.venv/lib/python3.10/site-packages/triton/backends/nvidia/compiler.py:341\u001b[0m, in \u001b[0;36mCUDABackend.make_llir\u001b[0;34m(self, src, metadata, options, capability)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRITON_DISABLE_LINE_INFO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    340\u001b[0m     passes\u001b[38;5;241m.\u001b[39mllvmir\u001b[38;5;241m.\u001b[39madd_di_scope(pm)\n\u001b[0;32m--> 341\u001b[0m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# LLVM-IR (MLIR) -> LLVM-IR (LLVM)\u001b[39;00m\n\u001b[1;32m    343\u001b[0m llvm\u001b[38;5;241m.\u001b[39minit_targets()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    O_block,\n",
    "    l_i,\n",
    "    m_i,\n",
    "    Q_block,\n",
    "    K_block_ptr,\n",
    "    V_block_ptr,\n",
    "    block_index_q,\n",
    "    softmax_scale,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "    offs_q: tl.constexpr,\n",
    "    offs_kv: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "):\n",
    "    # range of values handled by this stage\n",
    "    if STAGE == 1:\n",
    "        # From 0 to the left of the diagonal\n",
    "        lo, hi = 0, block_index_q * BLOCK_SIZE_Q\n",
    "    elif STAGE == 2:\n",
    "        # Used only for the block in which there is transition between non-masked and masked keys\n",
    "        lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1) * BLOCK_SIZE_Q\n",
    "        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n",
    "    else:\n",
    "        # Only used for non-causal attention\n",
    "        lo, hi = 0, SEQ_LEN\n",
    "\n",
    "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
    "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
    "\n",
    "    # loop over k, v and update accumulator\n",
    "    for start_kv in range(lo, hi, BLOCK_SIZE_KV):\n",
    "        # Just let the compiler know that start_n is a multiple of BLOCK_N, so the compiler can do optimizations\n",
    "        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n",
    "\n",
    "        # -- compute qk ----\n",
    "        K_block = tl.load(K_block_ptr)\n",
    "        QK_block = tl.dot(Q_block, K_block)\n",
    "\n",
    "        if STAGE == 2:\n",
    "            mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])\n",
    "            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1.0e6)\n",
    "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1))\n",
    "            QK_block -= m_ij[:, None]\n",
    "        else:\n",
    "            # Compute the maximum value of qk or keep the old max value\n",
    "            m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)\n",
    "            QK_block = QK_block * softmax_scale - m_ij[:, None]\n",
    "\n",
    "        # Compute the exponential of each dot product, so now we are computing exp(qk_ij - m_ij)\n",
    "        P_block = tl.math.exp(QK_block)\n",
    "        # Compute the sum by rows of the attention scores\n",
    "        l_ij = tl.sum(P_block, 1)\n",
    "\n",
    "        # This is the correction factor for the previous l_i\n",
    "        alpha = tl.math.exp(m_i - m_ij)\n",
    "        # Apply the correction factor to the previous l_i and add the new l_ij\n",
    "        l_i = l_i * alpha + l_ij\n",
    "\n",
    "        V_block = tl.load(V_block_ptr)\n",
    "        P_block = P_block.to(tl.float16)\n",
    "        # This computes the following: O_new = P x V + O_old * alpha\n",
    "        O_block = O_block * alpha[:, None]\n",
    "        O_block = tl.dot(P_block, V_block, O_block)\n",
    "\n",
    "        m_i = m_ij\n",
    "\n",
    "        # Move to the next block of K and V\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0))\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV))\n",
    "    return O_block, l_i, m_i\n",
    "\n",
    "\n",
    "@triton.autotune(\n",
    "    [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_Q\": BLOCK_SIZE_Q, \"BLOCK_SIZE_KV\": BLOCK_SIZE_KV},\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "        for BLOCK_SIZE_Q in [16, 32, 64, 128]\n",
    "        for BLOCK_SIZE_KV in [16, 32, 64]\n",
    "        for num_stages in ([3, 4, 7])\n",
    "        for num_warps in [2, 4]\n",
    "    ],\n",
    "    key=[\"SEQ_LEN\", \"HEAD_DIM\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    Q,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    K,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    V,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    softmax_scale,\n",
    "    M,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN\n",
    "    O,  # BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM\n",
    "    stride_Q_batch,\n",
    "    stride_Q_head,\n",
    "    stride_Q_seq,\n",
    "    stride_Q_dim,\n",
    "    stride_K_batch,\n",
    "    stride_K_head,\n",
    "    stride_K_seq,\n",
    "    stride_K_dim,\n",
    "    stride_V_batch,\n",
    "    stride_V_head,\n",
    "    stride_V_seq,\n",
    "    stride_V_dim,\n",
    "    stride_O_batch,\n",
    "    stride_O_head,\n",
    "    stride_O_seq,\n",
    "    stride_O_dim,\n",
    "    BATCH_SIZE,\n",
    "    NUM_HEADS: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    HEAD_DIM: tl.constexpr,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n",
    "\n",
    "    # This indicate which block in the sequence length to process\n",
    "    block_index_q = tl.program_id(0)\n",
    "\n",
    "    # This indicates which head and batch to process. Each program is associated with a single head of a single batch\n",
    "    index_batch_head = tl.program_id(1)\n",
    "    # This indicate which batch this program is associated with (each batch has NUM_HEADS heads)\n",
    "    index_batch = index_batch_head // NUM_HEADS\n",
    "    # This indicate the position of the head in the batch\n",
    "    index_head = index_batch_head % NUM_HEADS\n",
    "\n",
    "    # This allows to get the (N_CTX, HEAD_DIM) block in the Q, K, V by selecting indexing it by batch and head\n",
    "    qvk_offset = (\n",
    "        index_batch.to(tl.int64) * stride_Q_batch\n",
    "        + index_head.to(tl.int64) * stride_Q_head\n",
    "    )\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q + qvk_offset,\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_Q_seq, stride_Q_dim),\n",
    "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V + qvk_offset,\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_V_seq, stride_V_dim),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K + qvk_offset,\n",
    "        shape=(HEAD_DIM, SEQ_LEN),\n",
    "        strides=(\n",
    "            stride_K_dim,\n",
    "            stride_K_seq,\n",
    "        ),  # We invert the strides w.r.t Q, so we transpose the matrix\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O + qvk_offset,\n",
    "        shape=(SEQ_LEN, HEAD_DIM),\n",
    "        strides=(stride_O_seq, stride_O_dim),\n",
    "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # offs_q: the offsets for the tokens in the Q to process\n",
    "    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n",
    "    # offs_kv: the offsets for the tokens in the K and V sequence to process\n",
    "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
    "\n",
    "    # m_i: the running maximum. We have one for each query\n",
    "    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")\n",
    "    # l_i: the running sum. We have one for each query (as we sum the attention scores by rows)\n",
    "    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0\n",
    "    # acc: the accumulator for the output, which is a group of rows of the O matrix\n",
    "    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # load the blocks of Q: it will stay in SRAM throughout\n",
    "    Q_block = tl.load(Q_block_ptr)\n",
    "\n",
    "    # Stage: 3 if causal, else 1\n",
    "\n",
    "    if STAGE == 1 or STAGE == 3:\n",
    "        # This step runs for non-causal attention or for the blocks to the left of the diagonal in the causal attention\n",
    "        O_block, l_i, m_i = _attn_fwd_inner(\n",
    "            O_block,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            Q_block,\n",
    "            K_block_ptr,\n",
    "            V_block_ptr,\n",
    "            block_index_q,\n",
    "            softmax_scale,\n",
    "            BLOCK_SIZE_Q,\n",
    "            BLOCK_SIZE_KV,\n",
    "            4 - STAGE,\n",
    "            offs_q,\n",
    "            offs_kv,\n",
    "            SEQ_LEN,\n",
    "        )\n",
    "\n",
    "    if STAGE == 3:\n",
    "        # This step runs for the blocks to the right of the diagonal in the causal attention\n",
    "        O_block, l_i, m_i = _attn_fwd_inner(\n",
    "            O_block,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            Q_block,\n",
    "            K_block_ptr,\n",
    "            V_block_ptr,\n",
    "            block_index_q,\n",
    "            softmax_scale,\n",
    "            BLOCK_SIZE_Q,\n",
    "            BLOCK_SIZE_KV,\n",
    "            2,\n",
    "            offs_q,\n",
    "            offs_kv,\n",
    "            SEQ_LEN,\n",
    "        )\n",
    "    # epilogue\n",
    "    m_i += tl.math.log(\n",
    "        l_i\n",
    "    )  # This is needed to compute the logsumexp for the backwards pass\n",
    "    O_block = O_block / l_i[:, None]\n",
    "    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q\n",
    "    tl.store(m_ptrs, m_i)\n",
    "    tl.store(O_block_ptr, O_block.to(O.type.element_ty))\n",
    "\n",
    "\n",
    "\n",
    "class TritonAttention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, causal, softmax_scale):\n",
    "        HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
    "        HEAD_DIM_V = V.shape[-1]\n",
    "\n",
    "        BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM = Q.shape\n",
    "\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        stage = 3 if causal else 1\n",
    "\n",
    "        grid = lambda args: (\n",
    "            triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]),\n",
    "            BATCH_SIZE * NUM_HEADS,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # M is the logsumexp for the backward pass, one for each query\n",
    "        M = torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        _attn_fwd[grid](\n",
    "            Q=Q,\n",
    "            K=K,\n",
    "            V=V,\n",
    "            softmax_scale=softmax_scale,\n",
    "            M=M,\n",
    "            O=O,\n",
    "            stride_Q_batch=Q.stride(0),\n",
    "            stride_Q_head=Q.stride(1),\n",
    "            stride_Q_seq=Q.stride(2),\n",
    "            stride_Q_dim=Q.stride(3),\n",
    "            stride_K_batch=K.stride(0),\n",
    "            stride_K_head=K.stride(1),\n",
    "            stride_K_seq=K.stride(2),\n",
    "            stride_K_dim=K.stride(3),\n",
    "            stride_V_batch=V.stride(0),\n",
    "            stride_V_head=V.stride(1),\n",
    "            stride_V_seq=V.stride(2),\n",
    "            stride_V_dim=V.stride(3),\n",
    "            stride_O_batch=O.stride(0),\n",
    "            stride_O_head=O.stride(1),\n",
    "            stride_O_seq=O.stride(2),\n",
    "            stride_O_dim=O.stride(3),\n",
    "            BATCH_SIZE=Q.shape[0],\n",
    "            NUM_HEADS=Q.shape[1],\n",
    "            SEQ_LEN=Q.shape[2],\n",
    "            HEAD_DIM=HEAD_DIM_K,\n",
    "            STAGE=stage,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, O, M)\n",
    "        ctx.grid = grid\n",
    "        ctx.softmax_scale = softmax_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, O, M = ctx.saved_tensors\n",
    "\n",
    "        return None, None, None, None, None\n",
    "\n",
    "\n",
    "def test_op(BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM, causal, dtype=torch.float16):\n",
    "    Q = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "    K = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "    V = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "\n",
    "    softmax_scale = 1 / (HEAD_DIM**0.5)\n",
    "    dO = torch.randn_like(Q)\n",
    "\n",
    "    # reference implementation\n",
    "    MASK = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=\"cuda\"))\n",
    "    P = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale\n",
    "    if causal:\n",
    "        P[:, :, MASK == 0] = float(\"-inf\")\n",
    "    P = torch.softmax(P.float(), dim=-1).half()\n",
    "    ref_O = torch.matmul(P, V)\n",
    "\n",
    "    # triton implementation\n",
    "    tri_out = TritonAttention.apply(Q, K, V, causal, softmax_scale).half()\n",
    "\n",
    "    # compare\n",
    "    rtol = 0.0\n",
    "    atol = 1e-2\n",
    "    assert torch.allclose(ref_O, tri_out, atol=atol, rtol=rtol)\n",
    "\n",
    "test_op(BATCH_SIZE=2, NUM_HEADS=1, SEQ_LEN=1024, HEAD_DIM=1024, causal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d18b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Benchmarking Causal Attention\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished benchmarking Naive PyTorch        | Fwd: 42.596 ms | Bwd: 63.943 ms | Total: 106.539 ms | Peak Memory:  5784.25 MiB\n",
      "Finished benchmarking Triton Flash         | Fwd:  0.645 ms | Bwd:  5.908 ms | Total:  6.553 ms | Peak Memory:   137.00 MiB\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Benchmarking Non-Causal Attention\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Finished benchmarking Naive PyTorch        | Fwd: 31.466 ms | Bwd: 54.526 ms | Total: 85.992 ms | Peak Memory:  5752.25 MiB\n",
      "Finished benchmarking Triton Flash         | Fwd:  1.378 ms | Bwd:  6.068 ms | Total:  7.445 ms | Peak Memory:   137.00 MiB\n",
      "\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "                              Flash Attention Benchmark Results\n",
      "====================================================================================================\n",
      "+----------------+------------+--------------------+--------------------+--------------------+-------------------+\n",
      "| Implementation |    Type    |      Fwd (ms)      |      Bwd (ms)      |     Total (ms)     | Peak Memory (MiB) |\n",
      "+----------------+------------+--------------------+--------------------+--------------------+-------------------+\n",
      "| Naive PyTorch  |   Causal   | 42.596351623535156 | 63.942657470703125 | 106.53900909423828 |      5784.25      |\n",
      "|  Triton Flash  |   Causal   | 0.6448392262381892 | 5.907849898705115  | 6.552689124943305  |       137.0       |\n",
      "| Naive PyTorch  | Non-Causal | 31.46615473429362  | 54.52595138549805  | 85.99210611979167  |      5752.25      |\n",
      "|  Triton Flash  | Non-Causal |  1.37784643629764  | 6.067626714706421  | 7.445473151004061  |       137.0       |\n",
      "+----------------+------------+--------------------+--------------------+--------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "def naive_attention(Q, K, V, causal, softmax_scale):\n",
    "    \"\"\"\n",
    "    A standard, memory-inefficient attention implementation.\n",
    "    \"\"\"\n",
    "    _, _, SEQ_LEN, _ = Q.shape\n",
    "    # (B, H, S, S)\n",
    "    P = torch.matmul(Q, K.transpose(2, 3)) * softmax_scale\n",
    "    if causal:\n",
    "        mask = torch.tril(torch.ones((SEQ_LEN, SEQ_LEN), device=Q.device)).bool()\n",
    "        P = P.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    P = torch.softmax(P.float(), dim=-1).to(Q.dtype)\n",
    "    # (B, H, S, D)\n",
    "    ref_O = torch.matmul(P, V)\n",
    "    return ref_O\n",
    "\n",
    "\n",
    "def benchmark(func, Q, K, V, dO, causal, softmax_scale, func_name):\n",
    "    \"\"\"\n",
    "    Runs a benchmark for a given attention function using triton.testing.do_bench.\n",
    "\n",
    "    Args:\n",
    "        func: The attention function to test.\n",
    "        Q, K, V, dO: Input tensors.\n",
    "        causal: Boolean flag for causal masking.\n",
    "        softmax_scale: Scaling factor for attention.\n",
    "        func_name: A string name for the function for reporting.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing (fwd_ms, bwd_ms, total_ms, peak_memory_mib).\n",
    "    \"\"\"\n",
    "    # Use a lambda to pass arguments to the function being benchmarked\n",
    "    forward_pass = lambda: func(Q, K, V, causal, softmax_scale)\n",
    "\n",
    "    # --- Time the forward pass ---\n",
    "    # `do_bench` returns the median time in ms\n",
    "    fwd_latency_ms = triton.testing.do_bench(forward_pass)\n",
    "\n",
    "    # --- Time the backward pass ---\n",
    "    # We need to run a forward pass first to get the output for the backward call\n",
    "    output = func(Q, K, V, causal, softmax_scale)\n",
    "    # The backward pass function needs to be a zero-argument lambda\n",
    "    backward_pass = lambda: output.backward(dO, retain_graph=True)\n",
    "    bwd_latency_ms = triton.testing.do_bench(backward_pass)\n",
    "\n",
    "    # --- Measure peak memory ---\n",
    "    # We run one full forward-backward pass to measure memory\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats(\"cuda\")\n",
    "    # We need to clone inputs to avoid gradient accumulation issues from previous runs\n",
    "    q_clone, k_clone, v_clone = Q.clone(), K.clone(), V.clone()\n",
    "    output_mem = func(q_clone, k_clone, v_clone, causal, softmax_scale)\n",
    "    output_mem.backward(dO)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    peak_memory_bytes = torch.cuda.max_memory_allocated(\"cuda\")\n",
    "    peak_memory_mib = peak_memory_bytes / (1024 * 1024)\n",
    "    total_latency_ms = fwd_latency_ms + bwd_latency_ms\n",
    "\n",
    "    print(\n",
    "        f\"Finished benchmarking {func_name:<20} | \"\n",
    "        f\"Fwd: {fwd_latency_ms:6.3f} ms | \"\n",
    "        f\"Bwd: {bwd_latency_ms:6.3f} ms | \"\n",
    "        f\"Total: {total_latency_ms:6.3f} ms | \"\n",
    "        f\"Peak Memory: {peak_memory_mib:8.2f} MiB\"\n",
    "    )\n",
    "    return fwd_latency_ms, bwd_latency_ms, total_latency_ms, peak_memory_mib\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 4\n",
    "NUM_HEADS = 1\n",
    "SEQ_LEN = 1024\n",
    "HEAD_DIM = 1024\n",
    "DTYPE = torch.float16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# --- Create Tensors ---\n",
    "Q = (\n",
    "    torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=DTYPE, device=DEVICE\n",
    "    )\n",
    "    .normal_(mean=0.0, std=0.5)\n",
    "    .requires_grad_(False)\n",
    ")\n",
    "K = (\n",
    "    torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=DTYPE, device=DEVICE\n",
    "    )\n",
    "    .normal_(mean=0.0, std=0.5)\n",
    "    .requires_grad_(False)\n",
    ")\n",
    "V = (\n",
    "    torch.empty(\n",
    "        (BATCH_SIZE, NUM_HEADS, SEQ_LEN, HEAD_DIM), dtype=DTYPE, device=DEVICE\n",
    "    )\n",
    "    .normal_(mean=0.0, std=0.5)\n",
    "    .requires_grad_(False)\n",
    ")\n",
    "dO = torch.randn_like(Q)\n",
    "softmax_scale = 1 / (HEAD_DIM**0.5)\n",
    "\n",
    "# --- Run Benchmarks ---\n",
    "results = []\n",
    "for causal_flag in [True, False]:\n",
    "    causal_str = \"Causal\" if causal_flag else \"Non-Causal\"\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Benchmarking {causal_str} Attention\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    # Benchmark Naive Implementation\n",
    "    fwd, bwd, total, mem = benchmark(\n",
    "        naive_attention, Q, K, V, dO, causal_flag, softmax_scale, \"Naive PyTorch\"\n",
    "    )\n",
    "    results.append([\"Naive PyTorch\", causal_str, fwd, bwd, total, mem])\n",
    "\n",
    "    # Benchmark Triton Implementation\n",
    "    fwd, bwd, total, mem = benchmark(\n",
    "        TritonAttention.apply,\n",
    "        Q,\n",
    "        K,\n",
    "        V,\n",
    "        dO,\n",
    "        causal_flag,\n",
    "        softmax_scale,\n",
    "        \"Triton Flash\",\n",
    "    )\n",
    "    results.append([\"Triton Flash\", causal_str, fwd, bwd, total, mem])\n",
    "\n",
    "# --- Print Final Table ---\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"\\n\" * 2)\n",
    "print(\"=\" * 100)\n",
    "print(\" \" * 30 + \"Flash Attention Benchmark Results\")\n",
    "print(\"=\" * 100)\n",
    "print(\n",
    "    tabulate(\n",
    "        results,\n",
    "        headers=[\n",
    "            \"Implementation\",\n",
    "            \"Type\",\n",
    "            \"Fwd (ms)\",\n",
    "            \"Bwd (ms)\",\n",
    "            \"Total (ms)\",\n",
    "            \"Peak Memory (MiB)\",\n",
    "        ],\n",
    "        tablefmt=\"pretty\",\n",
    "        floatfmt=\".3f\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49bfeea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
