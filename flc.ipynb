{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "298ef124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refernce is complete\n",
      "triton is complete\n",
      "Failed ...\n",
      "Differences (abs > atol):\n",
      "Indices: (tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'), tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0'), tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n",
      "       device='cuda:0'))\n",
      "Reference values:\n",
      " tensor([  3.7129,  45.9375, -20.0000, -43.8125, -25.3438, -29.8594,   7.2305,\n",
      "         46.4688, -35.1562, -48.3125], device='cuda:0', dtype=torch.float16)\n",
      "Triton values:\n",
      " tensor([  3.6875,  45.5938, -19.8594, -43.5000, -25.1719, -29.6406,   7.1797,\n",
      "         46.1250, -34.9375, -47.9688], device='cuda:0', dtype=torch.float16)\n",
      "Diffs:\n",
      " tensor([0.0254, 0.3438, 0.1406, 0.3125, 0.1719, 0.2188, 0.0508, 0.3438, 0.2188,\n",
      "        0.3438], device='cuda:0', dtype=torch.float16)\n",
      "Sums diff\n",
      "tensor([[[2.0871e-04],\n",
      "         [1.8287e-04],\n",
      "         [1.1675e-03],\n",
      "         [3.0988e-04],\n",
      "         [4.8286e-04],\n",
      "         [3.2783e-06],\n",
      "         [1.6522e-04],\n",
      "         [6.3121e-05],\n",
      "         [1.0979e-04],\n",
      "         [1.7703e-04],\n",
      "         [6.7353e-05],\n",
      "         [1.7065e-04],\n",
      "         [9.7013e-04],\n",
      "         [1.7655e-04],\n",
      "         [2.2942e-04],\n",
      "         [1.5378e-05]]], device='cuda:0')\n",
      "Cosim diff\n",
      "tensor([[[7.3242e-04, 9.7656e-04, 0.0000e+00, 3.0518e-04, 6.1035e-05,\n",
      "          4.8828e-04, 0.0000e+00, 0.0000e+00, 9.7656e-04, 1.2207e-04,\n",
      "          0.0000e+00, 9.7656e-04, 1.2207e-04, 9.7656e-04, 4.8828e-04,\n",
      "          0.0000e+00],\n",
      "         [1.2207e-04, 0.0000e+00, 4.5776e-05, 0.0000e+00, 2.4414e-04,\n",
      "          0.0000e+00, 6.1035e-05, 0.0000e+00, 4.8828e-04, 1.2207e-04,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8828e-04],\n",
      "         [0.0000e+00, 4.8828e-04, 1.6785e-04, 0.0000e+00, 2.4414e-04,\n",
      "          4.8828e-04, 2.4414e-04, 3.0518e-05, 9.7656e-04, 2.4414e-04,\n",
      "          4.8828e-04, 4.8828e-04, 2.4414e-04, 9.7656e-04, 4.8828e-04,\n",
      "          9.7656e-04],\n",
      "         [4.8828e-04, 0.0000e+00, 4.8828e-04, 1.3733e-04, 0.0000e+00,\n",
      "          1.2207e-04, 4.8828e-04, 0.0000e+00, 4.8828e-04, 2.4414e-04,\n",
      "          9.7656e-04, 9.7656e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8828e-04],\n",
      "         [0.0000e+00, 2.4414e-04, 4.8828e-04, 7.6294e-05, 0.0000e+00,\n",
      "          0.0000e+00, 9.7656e-04, 6.1035e-05, 6.1035e-05, 1.2207e-04,\n",
      "          4.8828e-04, 0.0000e+00, 2.4414e-04, 4.8828e-04, 6.1035e-05,\n",
      "          0.0000e+00],\n",
      "         [9.7656e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 4.8828e-04, 1.9531e-03, 1.9531e-03,\n",
      "          4.8828e-04, 0.0000e+00, 9.7656e-04, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [2.5391e-02, 3.4375e-01, 1.4062e-01, 4.8828e-03, 3.1250e-01,\n",
      "          1.7188e-01, 2.1875e-01, 5.0781e-02, 3.4375e-01, 2.1875e-01,\n",
      "          3.4375e-01, 3.4375e-01, 1.2500e-01, 2.5000e-01, 2.1875e-01,\n",
      "          1.5625e-01],\n",
      "         [0.0000e+00, 2.4414e-04, 0.0000e+00, 2.4414e-04, 6.1035e-05,\n",
      "          0.0000e+00, 0.0000e+00, 9.1553e-05, 2.4414e-04, 4.8828e-04,\n",
      "          0.0000e+00, 1.2207e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          6.1035e-05],\n",
      "         [2.4414e-04, 3.0518e-04, 0.0000e+00, 1.2207e-04, 6.1035e-05,\n",
      "          3.0518e-04, 4.8828e-04, 0.0000e+00, 4.8828e-04, 1.2207e-04,\n",
      "          1.2207e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4414e-04, 1.9531e-03,\n",
      "          0.0000e+00, 9.7656e-04, 9.7656e-04, 2.4414e-04, 2.4414e-04,\n",
      "          9.7656e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.4414e-04,\n",
      "          0.0000e+00],\n",
      "         [0.0000e+00, 1.2207e-04, 0.0000e+00, 0.0000e+00, 6.1035e-05,\n",
      "          3.0518e-05, 0.0000e+00, 0.0000e+00, 4.5776e-05, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2207e-04,\n",
      "          0.0000e+00],\n",
      "         [1.2207e-04, 0.0000e+00, 1.2207e-04, 0.0000e+00, 0.0000e+00,\n",
      "          4.8828e-04, 2.4414e-04, 0.0000e+00, 9.7656e-04, 2.4414e-04,\n",
      "          9.7656e-04, 4.8828e-04, 0.0000e+00, 0.0000e+00, 2.4414e-04,\n",
      "          0.0000e+00],\n",
      "         [4.8828e-04, 1.9531e-03, 4.8828e-04, 1.2207e-04, 4.8828e-04,\n",
      "          2.4414e-04, 1.6403e-04, 1.2207e-04, 1.9531e-03, 0.0000e+00,\n",
      "          2.4414e-04, 4.8828e-04, 0.0000e+00, 9.7656e-04, 4.8828e-04,\n",
      "          4.8828e-04],\n",
      "         [2.4414e-04, 0.0000e+00, 2.4414e-04, 0.0000e+00, 6.1035e-05,\n",
      "          4.8828e-04, 2.4414e-04, 2.4414e-04, 0.0000e+00, 1.8311e-04,\n",
      "          1.2207e-04, 2.4414e-04, 1.2207e-04, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          2.4414e-04, 0.0000e+00, 0.0000e+00, 9.7656e-04, 0.0000e+00,\n",
      "          9.7656e-04, 0.0000e+00, 6.1035e-05, 1.9531e-03, 9.7656e-04,\n",
      "          0.0000e+00],\n",
      "         [2.4414e-04, 0.0000e+00, 0.0000e+00, 3.6621e-04, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00, 2.4414e-04, 0.0000e+00, 2.4414e-04,\n",
      "          0.0000e+00, 0.0000e+00, 2.4414e-04, 3.6621e-04, 0.0000e+00,\n",
      "          4.8828e-04]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "@triton.jit\n",
    "def _ctxt_fwd_inner(\n",
    "    O_block,\n",
    "    s_i,\n",
    "    Q_block,\n",
    "    K_block_ptr,\n",
    "    V_block_ptr,\n",
    "    P_block_ptr,\n",
    "    index_block_q,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "    offs_q: tl.constexpr,\n",
    "    offs_kv: tl.constexpr,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "):\n",
    "    if STAGE == 1:\n",
    "        lo, hi = 0, index_block_q * BLOCK_SIZE_Q\n",
    "    elif STAGE == 2:\n",
    "        lo, hi = index_block_q * BLOCK_SIZE_Q, (index_block_q + 1) * BLOCK_SIZE_Q\n",
    "        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n",
    "    else:\n",
    "        lo, hi = 0, SEQ_LEN\n",
    "\n",
    "    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n",
    "    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n",
    "\n",
    "    for start_kv in tl.range(lo, hi, BLOCK_SIZE_KV):\n",
    "        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n",
    "\n",
    "        K_block  = tl.load(K_block_ptr)\n",
    "        QK_block = tl.dot(Q_block, K_block)\n",
    "        \n",
    "        if STAGE == 2:\n",
    "            mask = offs_q[:, None] >= (start_kv + offs_kv[None, :])\n",
    "            QK_block = QK_block * mask\n",
    "\n",
    "        s_ij = tl.sum(QK_block, axis=1) + s_i\n",
    "\n",
    "        V_block = tl.load(V_block_ptr)\n",
    "        # QK_block = QK_block.to(tl.float16)\n",
    "        # O_block = tl.dot(QK_block, V_block, O_block)\n",
    "        O_block = tl.dot(QK_block.to(V_block.dtype), V_block, O_block)\n",
    "\n",
    "        s_i = s_ij\n",
    "\n",
    "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0))\n",
    "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV))\n",
    "        # P_block_ptr = tl.advance(P_block_ptr, (0, BLOCK_SIZE_KV))\n",
    "    \n",
    "    return O_block, s_i\n",
    "\n",
    "@triton.autotune(\n",
    "    [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_Q\": BLOCK_SIZE_Q, \"BLOCK_SIZE_KV\": BLOCK_SIZE_KV},\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps,\n",
    "        )\n",
    "        for BLOCK_SIZE_Q in [16]\n",
    "        for BLOCK_SIZE_KV in [16]\n",
    "        for num_stages in ([3])\n",
    "        for num_warps in [2]\n",
    "    ],\n",
    "    key=[\"SEQ_LEN\", \"DIM\"],\n",
    ")\n",
    "@triton.jit\n",
    "def _ctxt_fwd(\n",
    "    Q,  # BATCH_SIZE, SEQ_LEN, DIM\n",
    "    K,  # BATCH_SIZE, SEQ_LEN, DIM\n",
    "    V,  # BATCH_SIZE, SEQ_LEN, DIM\n",
    "    O,  # BATCH_SIZE, SEQ_LEN, DIM\n",
    "    P,  # SEQ_LEN, SEQ_LEN\n",
    "    S,  # BATCH_SIZE, SEQ_LEN\n",
    "    stride_Q_batch,\n",
    "    stride_Q_seq,\n",
    "    stride_Q_dim,\n",
    "    stride_K_batch,\n",
    "    stride_K_seq,\n",
    "    stride_K_dim,\n",
    "    stride_V_batch,\n",
    "    stride_V_seq,\n",
    "    stride_V_dim,\n",
    "    stride_O_batch,\n",
    "    stride_O_seq,\n",
    "    stride_O_dim,\n",
    "    stride_P_row,\n",
    "    stride_P_col,\n",
    "    BATCH_SIZE,\n",
    "    SEQ_LEN: tl.constexpr,\n",
    "    DIM: tl.constexpr,\n",
    "    BLOCK_SIZE_Q: tl.constexpr,\n",
    "    BLOCK_SIZE_KV: tl.constexpr,\n",
    "    STAGE: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(BLOCK_SIZE_KV <= DIM)\n",
    "    \n",
    "    index_block_q = tl.program_id(0)\n",
    "    index_batch   = tl.program_id(1)\n",
    "\n",
    "    qkv_offset = index_batch.to(tl.int64) * stride_Q_batch\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        base=Q + qkv_offset,\n",
    "        shape=(SEQ_LEN, DIM),\n",
    "        strides=(stride_Q_seq, stride_Q_dim),\n",
    "        offsets=(index_block_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, DIM),\n",
    "        order=(1, 0), \n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        base=V + qkv_offset,\n",
    "        shape=(SEQ_LEN, DIM),\n",
    "        strides=(stride_V_seq, stride_V_dim),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_SIZE_KV, DIM),\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        base=K + qkv_offset,\n",
    "        shape=(DIM, SEQ_LEN),\n",
    "        # invert the strides w.r.t Q, so we transpose the matrix\n",
    "        strides=(stride_K_dim, stride_K_seq),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(DIM, BLOCK_SIZE_KV),\n",
    "        order=(0, 1),\n",
    "    )\n",
    "\n",
    "    P_block_ptr = tl.make_block_ptr(\n",
    "        base=P,\n",
    "        shape=(SEQ_LEN, SEQ_LEN),\n",
    "        strides=(stride_P_row, stride_P_col),\n",
    "        offsets=(index_block_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, BLOCK_SIZE_KV),\n",
    "        order=(1, 0)\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        base=O + qkv_offset,\n",
    "        shape=(SEQ_LEN, DIM),\n",
    "        strides=(stride_O_seq, stride_O_dim),\n",
    "        offsets=(index_block_q * BLOCK_SIZE_Q, 0),\n",
    "        block_shape=(BLOCK_SIZE_Q, DIM),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    offs_q  = index_block_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n",
    "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n",
    "\n",
    "    s_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32)\n",
    "    O_block = tl.zeros([BLOCK_SIZE_Q, DIM], dtype=tl.float32)\n",
    "\n",
    "    Q_block = tl.load(Q_block_ptr)\n",
    "\n",
    "    if STAGE == 1 or STAGE == 3:\n",
    "        # non-causal attention or blocks to the left of the diagonal in causal attention\n",
    "        O_block, s_i = _ctxt_fwd_inner(\n",
    "            O_block,\n",
    "            s_i,\n",
    "            Q_block,\n",
    "            K_block_ptr,\n",
    "            V_block_ptr,\n",
    "            P_block_ptr,\n",
    "            index_block_q,\n",
    "            BLOCK_SIZE_Q,\n",
    "            BLOCK_SIZE_KV,\n",
    "            4 - STAGE,\n",
    "            offs_q,\n",
    "            offs_kv,\n",
    "            SEQ_LEN,\n",
    "        )\n",
    "\n",
    "    if STAGE == 3:\n",
    "        # blocks to the right of the diagonal in causal attention\n",
    "        O_block, s_i = _ctxt_fwd_inner(\n",
    "            O_block,\n",
    "            s_i,\n",
    "            Q_block,\n",
    "            K_block_ptr,\n",
    "            V_block_ptr,\n",
    "            P_block_ptr,\n",
    "            index_block_q,\n",
    "            BLOCK_SIZE_Q,\n",
    "            BLOCK_SIZE_KV,\n",
    "            2,\n",
    "            offs_q,\n",
    "            offs_kv,\n",
    "            SEQ_LEN,\n",
    "        )\n",
    "    \n",
    "    O_block = O_block / (s_i[:, None] + 1e-8)\n",
    "    tl.store(O_block_ptr, O_block.to(O.type.element_ty))\n",
    "\n",
    "    s_ptrs = S + index_batch * SEQ_LEN + offs_q\n",
    "    tl.store(s_ptrs, s_i)\n",
    "\n",
    "class TritonCtxt(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, P, causal):\n",
    "        HEAD_DIM_Q, HEAD_DIM_K = Q.shape[-1], K.shape[-1]\n",
    "        HEAD_DIM_V = V.shape[-1]\n",
    "\n",
    "        BATCH_SIZE, SEQ_LEN, HEAD_DIM = Q.shape\n",
    "\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        O = torch.empty_like(Q)\n",
    "        stage = 3 if causal else 1\n",
    "\n",
    "        grid = lambda args: (\n",
    "            triton.cdiv(SEQ_LEN, args[\"BLOCK_SIZE_Q\"]),\n",
    "            BATCH_SIZE,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # S is the logsumexp for the backward pass, one for each query\n",
    "        S = torch.empty(\n",
    "            (BATCH_SIZE, SEQ_LEN), device=Q.device, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        _ctxt_fwd[grid](\n",
    "            Q=Q,\n",
    "            K=K,\n",
    "            V=V,\n",
    "            O=O,\n",
    "            P=P,\n",
    "            S=S,\n",
    "            stride_Q_batch=Q.stride(0),\n",
    "            stride_Q_seq=Q.stride(1),\n",
    "            stride_Q_dim=Q.stride(2),\n",
    "            stride_K_batch=K.stride(0),\n",
    "            stride_K_seq=K.stride(1),\n",
    "            stride_K_dim=K.stride(2),\n",
    "            stride_V_batch=V.stride(0),\n",
    "            stride_V_seq=V.stride(1),\n",
    "            stride_V_dim=V.stride(2),\n",
    "            stride_O_batch=O.stride(0),\n",
    "            stride_O_seq=O.stride(1),\n",
    "            stride_O_dim=O.stride(2),\n",
    "            stride_P_row=P.stride(0),\n",
    "            stride_P_col=P.stride(1),\n",
    "            BATCH_SIZE=Q.shape[0],\n",
    "            SEQ_LEN=Q.shape[1],\n",
    "            DIM=HEAD_DIM_K,\n",
    "            STAGE=stage,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward(Q, K, V, P, O, S)\n",
    "        ctx.grid = grid\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "        return O, S\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dO):\n",
    "        Q, K, V, P, O, S = ctx.saved_tensors\n",
    "        return None, None, None, None, None\n",
    "\n",
    "def test_op(BATCH_SIZE, SEQ_LEN, HEAD_DIM, causal, dtype=torch.float16):\n",
    "    Q = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "\n",
    "    K = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "    \n",
    "    V = (\n",
    "        torch.empty(\n",
    "            (BATCH_SIZE, SEQ_LEN, HEAD_DIM), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "\n",
    "    P = (\n",
    "        torch.empty(\n",
    "            (SEQ_LEN, SEQ_LEN), dtype=dtype, device=\"cuda\"\n",
    "        )\n",
    "        .normal_(mean=0.0, std=0.5)\n",
    "        .requires_grad_(False)\n",
    "    )\n",
    "    P = torch.nn.init.xavier_normal_(P)\n",
    "\n",
    "    norm = torch.sqrt(torch.sum(Q ** 2, dim=-1, keepdim=True) + 1e-8)\n",
    "    Q = Q / norm\n",
    "    # Q = torch.nn.functional.normalize(Q, dim=-1)\n",
    "    norm = torch.sqrt(torch.sum(K ** 2, dim=-1, keepdim=True) + 1e-8)\n",
    "    K = K / norm\n",
    "    # K = torch.nn.functional.normalize(K, dim=-1)\n",
    "\n",
    "    # reference implementation\n",
    "    cosim = torch.matmul(Q, K.transpose(-1, -2))\n",
    "    # row_sums = torch.tril(cosim).sum(dim=-1, keepdim=True)\n",
    "    row_sums = cosim.sum(dim=-1, keepdim=True)\n",
    "    cosim_scores = cosim / (row_sums + 1e-8)\n",
    "    ref_O = (cosim_scores) @ V\n",
    "    ref_no_norm = cosim @ V\n",
    "\n",
    "    print(\"refernce is complete\")\n",
    "\n",
    "    # triton implementation\n",
    "    tri_out, s = TritonCtxt.apply(Q, K, V, P, causal)\n",
    "    tri_out = tri_out.half()\n",
    "\n",
    "    print(\"triton is complete\")\n",
    "    # compare\n",
    "    rtol = 0.0\n",
    "    atol = 1e-2\n",
    "    torch.set_printoptions(threshold=float('inf'))\n",
    "    try:\n",
    "        assert torch.allclose(ref_O, tri_out, atol=atol, rtol=rtol)\n",
    "        print(\"Passed ...\")\n",
    "    except:\n",
    "        print(\"Failed ...\")\n",
    "        diff = (ref_O - tri_out).abs()\n",
    "        mask = diff > atol\n",
    "        print(\"Differences (abs > atol):\")\n",
    "        print(\"Indices:\", mask.nonzero(as_tuple=True))\n",
    "        print(\"Reference values:\\n\", ref_O[mask][:10])\n",
    "        print(\"Triton values:\\n\", tri_out[mask][:10])\n",
    "        print(\"Diffs:\\n\", diff[mask][:10])\n",
    "\n",
    "    # print(\"My sums\")\n",
    "    # print(s[:, :, None])\n",
    "    # print(\"Torch sums\")\n",
    "    # print(row_sums)\n",
    "    print(\"Sums diff\")\n",
    "    print((s[:, :, None] - row_sums).abs())\n",
    "    \n",
    "    # print(\"My cosim\")\n",
    "    # print(tri_out[:, :1, :])\n",
    "    # print(\"Torch cosim\")\n",
    "    # print(ref_no_norm[:, :1, :])\n",
    "    print(\"Cosim diff\")\n",
    "    print((ref_O[:, :, :] - tri_out[:, :, :]).abs())\n",
    "\n",
    "test_op(BATCH_SIZE=1, SEQ_LEN=16, HEAD_DIM=16, causal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1a01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def naive_contextualizer(Q, K, V, P, causal):\n",
    "    \"\"\"\n",
    "    The memory-inefficient naive implementation.\n",
    "    \"\"\"\n",
    "    cosim = torch.matmul(Q, K.transpose(-1, -2))\n",
    "    \n",
    "    # The user's formula uses torch.tril(P). If P is already causal,\n",
    "    # this is redundant. We assume P needs the mask applied.\n",
    "    if causal:\n",
    "        # We apply the causal mask to the P matrix before multiplication\n",
    "        masked_P = torch.tril(P)\n",
    "        scores = masked_P * cosim\n",
    "    else:\n",
    "        scores = P * cosim\n",
    "        \n",
    "    output = torch.matmul(scores, V)\n",
    "    return output\n",
    "\n",
    "# ==============================================================================\n",
    "# Benchmarking Code\n",
    "# ==============================================================================\n",
    "\n",
    "def benchmark_forward_pass(func, func_name, *args):\n",
    "    \"\"\"\n",
    "    Benchmarks the forward pass of a function for latency and memory.\n",
    "    \"\"\"\n",
    "    # --- Measure Latency using Triton's utility ---\n",
    "    # `do_bench` returns the median time in ms.\n",
    "    latency_ms = triton.testing.do_bench(lambda: func(*args))\n",
    "\n",
    "    # --- Measure Peak Memory ---\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.reset_peak_memory_stats(\"cuda\")\n",
    "    func(*args) # Run one forward pass to measure memory\n",
    "    torch.cuda.synchronize()\n",
    "    peak_memory_mib = torch.cuda.max_memory_allocated(\"cuda\") / (1024 * 1024)\n",
    "\n",
    "    print(\n",
    "        f\"Finished benchmarking {func_name:<20} | \"\n",
    "        f\"Latency: {latency_ms:6.3f} ms | \"\n",
    "        f\"Peak Memory: {peak_memory_mib:8.2f} MiB\"\n",
    "    )\n",
    "    return latency_ms, peak_memory_mib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a1f2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Benchmarking Causal Forward Pass\n",
      "--------------------------------------------------------------------------------\n",
      "Finished benchmarking Naive PyTorch        | Latency: 12.476 ms | Peak Memory:  1312.12 MiB\n",
      "Finished benchmarking Triton Custom        | Latency:  2.687 ms | Peak Memory:   160.25 MiB\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                    Forward Pass Benchmark Results\n",
      "================================================================================\n",
      "+----------------+--------+--------------------+-------------------+\n",
      "| Implementation |  Type  |    Latency (ms)    | Peak Memory (MiB) |\n",
      "+----------------+--------+--------------------+-------------------+\n",
      "| Naive PyTorch  | Causal | 12.475830895560128 |     1312.125      |\n",
      "| Triton Custom  | Causal | 2.6867278055711226 |      160.25       |\n",
      "+----------------+--------+--------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 4\n",
    "SEQ_LEN = 8192\n",
    "HEAD_DIM = 128\n",
    "DTYPE = torch.float16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# --- Create Tensors ---\n",
    "# Note: Triton kernel expects contiguous tensors\n",
    "Q = torch.randn(BATCH_SIZE, SEQ_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "V = torch.randn(BATCH_SIZE, SEQ_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "P = torch.randn(SEQ_LEN, SEQ_LEN, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "\n",
    "# --- Run Benchmarks ---\n",
    "results = []\n",
    "for causal_flag in [True]:\n",
    "    causal_str = \"Causal\" if causal_flag else \"Non-Causal\"\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Benchmarking {causal_str} Forward Pass\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Benchmark Naive Implementation\n",
    "    naive_latency, naive_memory = benchmark_forward_pass(\n",
    "        naive_contextualizer, \"Naive PyTorch\", Q, Q, V, P, causal_flag\n",
    "    )\n",
    "    results.append([\"Naive PyTorch\", causal_str, naive_latency, naive_memory])\n",
    "\n",
    "    # Benchmark Triton Implementation\n",
    "    triton_latency, triton_memory = benchmark_forward_pass(\n",
    "        TritonCtxt.apply, \"Triton Custom\", Q, Q, V, P, causal_flag\n",
    "    )\n",
    "    results.append([\"Triton Custom\", causal_str, triton_latency, triton_memory])\n",
    "\n",
    "# --- Print Final Table ---\n",
    "print(\"\\n\" * 2)\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"Forward Pass Benchmark Results\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    tabulate(\n",
    "        results,\n",
    "        headers=[\"Implementation\", \"Type\", \"Latency (ms)\", \"Peak Memory (MiB)\"],\n",
    "        tablefmt=\"pretty\",\n",
    "        floatfmt=\".3f\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2565b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Benchmarking Causal Forward Pass\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished benchmarking Naive PyTorch        | Latency: 48.971 ms | Peak Memory:  5176.12 MiB\n",
      "Finished benchmarking Triton Custom        | Latency: 10.375 ms | Peak Memory:   568.38 MiB\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                    Forward Pass Benchmark Results\n",
      "================================================================================\n",
      "+----------------+--------+--------------------+-------------------+\n",
      "| Implementation |  Type  |    Latency (ms)    | Peak Memory (MiB) |\n",
      "+----------------+--------+--------------------+-------------------+\n",
      "| Naive PyTorch  | Causal | 48.97075271606445  |     5176.125      |\n",
      "| Triton Custom  | Causal | 10.375074820085006 |      568.375      |\n",
      "+----------------+--------+--------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 4\n",
    "SEQ_LEN = 16384\n",
    "HEAD_DIM = 128\n",
    "DTYPE = torch.float16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# --- Create Tensors ---\n",
    "# Note: Triton kernel expects contiguous tensors\n",
    "Q = torch.randn(BATCH_SIZE, SEQ_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "V = torch.randn(BATCH_SIZE, SEQ_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "P = torch.randn(SEQ_LEN, SEQ_LEN, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "\n",
    "# --- Run Benchmarks ---\n",
    "results = []\n",
    "for causal_flag in [True]:\n",
    "    causal_str = \"Causal\" if causal_flag else \"Non-Causal\"\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Benchmarking {causal_str} Forward Pass\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Benchmark Naive Implementation\n",
    "    naive_latency, naive_memory = benchmark_forward_pass(\n",
    "        naive_contextualizer, \"Naive PyTorch\", Q, Q, V, P, causal_flag\n",
    "    )\n",
    "    results.append([\"Naive PyTorch\", causal_str, naive_latency, naive_memory])\n",
    "\n",
    "    # Benchmark Triton Implementation\n",
    "    triton_latency, triton_memory = benchmark_forward_pass(\n",
    "        TritonCtxt.apply, \"Triton Custom\", Q, Q, V, P, causal_flag\n",
    "    )\n",
    "    results.append([\"Triton Custom\", causal_str, triton_latency, triton_memory])\n",
    "\n",
    "# --- Print Final Table ---\n",
    "print(\"\\n\" * 2)\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"Forward Pass Benchmark Results\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    tabulate(\n",
    "        results,\n",
    "        headers=[\"Implementation\", \"Type\", \"Latency (ms)\", \"Peak Memory (MiB)\"],\n",
    "        tablefmt=\"pretty\",\n",
    "        floatfmt=\".3f\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 1\n",
    "SEQ_LEN = 32768\n",
    "HEAD_DIM = 128\n",
    "DTYPE = torch.float16\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# --- Create Tensors ---\n",
    "# Note: Triton kernel expects contiguous tensors\n",
    "Q = torch.randn(BATCH_SIZE, SEQ_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "V = torch.randn(BATCH_SIZE, SEQ_LEN, HEAD_DIM, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "P = torch.randn(SEQ_LEN, SEQ_LEN, dtype=DTYPE, device=DEVICE).contiguous()\n",
    "\n",
    "# --- Run Benchmarks ---\n",
    "results = []\n",
    "for causal_flag in [True]:\n",
    "    causal_str = \"Causal\" if causal_flag else \"Non-Causal\"\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Benchmarking {causal_str} Forward Pass\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Benchmark Naive Implementation\n",
    "    naive_latency, naive_memory = benchmark_forward_pass(\n",
    "        naive_contextualizer, \"Naive PyTorch\", Q, Q, V, P, causal_flag\n",
    "    )\n",
    "    results.append([\"Naive PyTorch\", causal_str, naive_latency, naive_memory])\n",
    "\n",
    "    # Benchmark Triton Implementation\n",
    "    triton_latency, triton_memory = benchmark_forward_pass(\n",
    "        TritonCtxt.apply, \"Triton Custom\", Q, Q, V, P, causal_flag\n",
    "    )\n",
    "    results.append([\"Triton Custom\", causal_str, triton_latency, triton_memory])\n",
    "\n",
    "# --- Print Final Table ---\n",
    "print(\"\\n\" * 2)\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"Forward Pass Benchmark Results\")\n",
    "print(\"=\" * 80)\n",
    "print(\n",
    "    tabulate(\n",
    "        results,\n",
    "        headers=[\"Implementation\", \"Type\", \"Latency (ms)\", \"Peak Memory (MiB)\"],\n",
    "        tablefmt=\"pretty\",\n",
    "        floatfmt=\".3f\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a089e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
